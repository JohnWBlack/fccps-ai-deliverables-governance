From:	Tom Colvin <thomas.j.colvin@gmail.com>
Sent:	Thursday, 19 February, 2026 20:18
To:	John Black
Subject:	Re: Research & Shared Baseline (WS-RSB): guidance and deliverable for 20 Feb
Attachments:	Shared Baseline v01.docx; Shared Baseline v01.pdf

Importance:	High

Categories:	FCCPS AI Advisory Committee

Hey John,

I'm not sure how close this comes to answering the mail. In the time available, we were 
more able to gather resources than to read them. 

We directly answered the mail on key terms and classroom examples. We have a large list 
of resources, however getting them annotated is going to be a work in progress. Some of 
them, we wrote the relevant summaries in the main text; however, most of them are in an 
appendix at the moment.

As we discussed over email, the assumptions, misconceptions, and a few of the 
student/teacher use cases were distributed into Purpose (what we're trying to do), 
Principles (how we will try to do it), and Challenges (what makes creating a good policy 
hard). The positive framing for the misconceptions are that they are real risks / challenges 
that the policy has to grapple with. Likewise, the concept of misconceptions and what's 
reliable / brittle have been started in the evidence-table format. I stand by that it is 
premature / oversimplified to say that any of these things are good / bad / reliable / brittle 
without being able to point to evidence. My opinions on them, and the opinions of the 
broader group, should not be a sufficient basis for assessing what are essentially empirical 
claims. So my hope is that in the discussion tomorrow, I can ask folks to take a look on 
their own time, suggest potential functions of AI that should be added to the list and to 
provide any research / evidence they are aware of to act as positives or negatives for the AI 
function getting used in K-12 education. Elizabeth and I can keep updating the list as the 
weeks roll on.

Attaching the living document as it stands now as docx and pdf. This is ultimately hosted 
on google drive, so we could also give people a read-only link since i think that's easier to 
digest than Miro, which i admit I have a hard time following. Miro and similar infinite-
whiteboard services are challenging for me navigate, but maybe that's just me. Anyways, if 
it's not a violation of any policy and you think it would be helpful, i can share a read-only 
link, let me know.

Cheers and see you tomorrow!
Tom 

On Mon, Feb 16, 2026 at 10:39?PM John Black <john.black@outlook.com> wrote:
Hi Tom.
 
Thank you. Your pushback here is exactly the right kind (and I don’t want you to feel like 
you need to “spin” anything).
 
A few quick clarifications to de-stress this:
 
*	“Misconceptions” = high-salience claims/concerns we must address, not 
things we can definitively debunk. For items like “AI replaces teachers / hurts 
learning / makes us dumber,” it’s totally OK to frame them as real risks. The 
“corrective framing” can simply be: what part may be true, under what conditions, 
and what guardrails / policy posture we use to manage it.
 
*	On “reliable vs brittle”: I’m not asking for “AI is always correct.” I mean lower-risk 
patterns when paired with guardrails (e.g., low stakes, human verification, 
process evidence, no sensitive data). It’s fine—even helpful—to call out where 
something looks reliable but can fail badly or harm learning outcomes.
 
*	Your evidence-table idea is perfect for Meeting 3. A “function ? potential upside 
? potential downside ? evidence (strong/mixed/limited) ? guardrails” table is an 
excellent way to keep this evidence-informed without overclaiming. If you can get 
even 3–5 rows to v1, that’s plenty—we can iterate.
 
Net: bring what you and Elizabeth have by Friday; partial is fine. Our goal on 2/20 is to 
reduce uncertainty enough to converge on values/principles, not to eliminate uncertainty 
entirely.
 
Best, 
John
 
 
From: Tom Colvin <thomas.j.colvin@gmail.com>  
Sent: Monday, 16 February, 2026 22:05 
To: John Black <john.black@outlook.com> 
Subject: Re: Research & Shared Baseline (WS-RSB): guidance and deliverable for 20 Feb
 
Thanks for the swift response! My response.
 
On Mon, Feb 16, 2026 at 8:21?PM John Black <john.black@outlook.com> wrote:
Hi Tom,
 
Thanks for the update — and for meeting with Elizabeth over the weekend to push this 
forward. It sounds like you’re in a good place. A few clarifications on each of the 
questions you raised:
 
1) Stickies: “assumptions” vs. “misconceptions”
You’re right that “misconception” can imply “provably wrong,” which isn’t always the 
case. Please feel free to treat these as common beliefs/claims we’re likely to hear 
(some wrong, some oversimplified, some genuinely contested) that we should address 
explicitly in policy + comms/PD. If something can’t be definitively proven wrong, that’s 
still useful — it becomes part of our shared baseline of “what people think / what needs 
clarification.”
Your refactor into purpose / principles / challenges makes total sense and is helpful. 
The only additional ask for Meeting 3 is: can you still surface a ranked list of Tier 1 items 
(e.g., ~5) with a one-sentence “what we should say instead” framing for each, plus a 
short Tier 2 list? That ranked list will be especially useful as we converge on 
values/principles.
I have a hard time seeing how to come up with a pithy reframing that is truthful. For 
example, three of the 'misconceptions' were
“AI 
repl
ace
s 
teac
hers
” — 
5 
dots
“AI 
HU
RT
S / 
imp
ede
s 
LEA
RNI
NG” 
— 3 
dots
“AI 
will 
mak
e us 
DU
MB
ER” 
— 2 
dots
Each of those may indeed be true and we can find evidence for them in the literature. The 
only real way to reframe them is simply to make them less absolute. "AI might replace 
teachers. AI can impede learning. AI can make us dumber." Whether these things become 
true or not in FCCPS depends on the way AI is integrated into the school. It is premature to 
say that these are wrong or to try to create a more benign framing of these things. The way 
to reframe them are as challenges that the policy and implementation must attempt to 
address.
 
2) “AI capabilities and limits” --what I meant
Agreed: a list of modalities (text/image/audio/etc.) isn’t the most helpful output here. 
What I’m looking for is plain-language, K-12-relevant guidance on what’s reliable vs. 
brittle and why it matters for schools.
Examples of the style that would be most useful:
*	“Reliable-ish” use cases (when paired with human judgment): drafting/revising 
text, brainstorming, generating examples, summarizing with a source in hand, 
first-pass differentiation, practice questions, etc.
*	Common failure modes / brittleness: confidently wrong outputs 
(“hallucinations”), weak citations, prompt sensitivity, bias/stereotypes, uneven 
reasoning, math/logic errors, and rapid model/version change.
*	Policy-relevant realities: privacy/records concerns; variability in output; and the 
fact that “detection” is not a dependable integrity strategy.
If you can pair each point with a quick “so what for policy/guidance” implication, that’s 
perfect.
From what perspective are you conceiving of reliability? An AI can reliably help a student 
generate ideas for an essay (brainstorming), but the research appears to show that is bad 
for learning outcomes. So for brainstorming, did you mean that an AI can reliably do the 
task (yes) or that it's reliably good for students (no)? Another related issue is that it really 
matters which AI you're using; math errors are effectively non-existent for an agent that 
can call tools, but are rampant in agents that don't use tools. That's a clear example, but it 
applies to pretty much everything; depending on the model you're using and the context 
it's been fed, all of the reliable-ish use cases you listed can fail miserably. I've had it revise 
my text in ways that fundamentally change meaning to make the output wrong. I've had 
them brainstorm ideas for me that were nonsense and provide examples of things that 
weren't true; a non-expert would probably have thought they were fine, but it's clear to me 
that they're deeply wrong. They can summarize sources when fed a document, but will 
often miss the important nuances that made the paper interesting. Etc.
 
I'm also not sure why we would list all the things an AI can "reliably" do unless we're 
implicitly suggesting that they're good for use in schools; e.g., we wouldn't say things like 
"AI can reliably make deepfakes of people you know" because that's not a thing we would 
want to highlight as a use case in school. However, we can't say any of the things on this 
list are good for use in schools until we find some research that says one way or the other. 
So it seems like such a list risks being just vibes and not reflective of the evidence-
informed approach that Bethany said would be used to create the policy.
 
As you can see, I'm not sure how to respond to this task. How are you planning to use this 
information? If I understand where this type of information is going, then maybe I can 
better see how to create it.
 
 
3) “Classroom reality” -- FCCPS vs. general vs. hypothetical
Please use hypothetical but plausible vignettes — they do not have to be validated as 
“this is exactly what FCCPS students are doing today.” The goal is to make the baseline 
concrete and shared, so we can draft principles and guardrails against real-looking 
situations.
3–5 short scenarios across:
1.	student use for learning,
2.	teacher use (instructional + professional), and
3.	assessment integrity “triage” (what’s allowed/expected vs. not).
If you can add a sentence on what changes by grade band (PK–2 / 3–5 / 6–8 / 9–12), even 
lightly, that will tee up the grade-band discussion.
Okay. So it sounds like a vignette that reflects how Alpha School has incorporated AI is not 
what you're after; they claim to have amazing results but they also have replaced 
"teachers" with a new role called "guides" that is basically there to keep the kids 
motivated to continue using the app where they learn. SAT scores are off the charts, but 
they no longer have a teacher-at-front-of-classroom model. I'm also keying in on your 
"guardrails" comment; I went to a falls church parents meeting on AI in schools a few 
weeks ago and heard some things that I know they are unhappy about, so I can work that 
in there.  
 
4) Your outline / scope
The outline you shared looks good. The only guidance I’d add is: for this checkpoint, 
please don’t feel pressure to fully build out longer sections (e.g., policy scan, broader 
research landscape, IB skills mapping) unless they’re already easy to include. If you have 
them, great -- but it’s also fine to keep those as brief bullets or an appendix and focus 
your “front page” effort on the definition-of-done items.
Cool. I imagine this is a document that Elizabeth and I will continue to build as we find 
more research and connect the findings to the outcomes that folks are hoping to create.  
 
5) Timing / stress level
Please don’t treat Tuesday EOD as a hard deadline. If you can send a “good enough for 
pre-reads” snapshot by Tue 17 Feb (even if some parts are bullets), I’ll use it. If that’s 
stressful, it’s completely fine to finish what you plan to present by Friday. The goal is 
clarity and usefulness, not perfection.
 
6) What to plan for on Friday (Meeting 3)
Let’s plan for ~8–10 minutes for you and Elizabeth to walk the committee through:
*	top 5–7 baseline takeaways (capabilities/limits),
*	Tier 1 misconceptions/beliefs list + corrective framing,
Need more guidance on these two if it's what you really want because, as i said before, it's 
not that clear cut. I'll think about it ... maybe i'm just getting hung up on the idea of 
'reliable'. I can see maybe a table like
 
This is what i was hoping to do anyways with the IB values. It's easier to do it with the IB 
values because there are simply fewer of them than possible use cases to articulate. 
Regardless, we could start this list and then have folks who know of certain research that 
they want to be captured send it to me and I'll populate the table. I plan to continue 
combing for studies as this subcommittee goes on so that we can know what the research 
says on as much as possible.
*	 
*	3 scenarios + grade-band notes, and
*	any “open questions” you want the committee to keep in mind as we pivot into 
Values/Principles.
We’ll do a quick Q&A, and then we’ll use your outputs directly as inputs for the Values & 
Principles convergence work.
Thanks again--This baseline is a key dependency for the rest of the workstreams, and 
what you’re describing is exactly what we need.
 
Best, 
John Black
 
 
From: Tom Colvin <thomas.j.colvin@gmail.com>  
Sent: Monday, 16 February, 2026 18:40 
To: John Black <john.black@outlook.com> 
Subject: Re: Research & Shared Baseline (WS-RSB): guidance and deliverable for 20 Feb
 
Hey John,
 
Elizabeth and I met up over the weekend and have been working on this. We have a few 
questions and comments before we finish up.
 
Stickies about Assumptions and Misconceptions. We think we know what you were 
trying to get at with these. Calling them 'misconceptions' implies that we can prove that 
they are wrong. Since we can't prove that, they are just more assumptions. Further, these 
assumptions / misconceptions were really pointing to either the intended purpose of the 
strategy, the principles by which the strategy should be created, and the challenges 
associated with creating a strategy that meets the purpose and principles. So we've 
refactored the sticky responses into those three categories and supplemented them with 
info from the FCCPS school board website.
 
AI capabilities and limits. Not sure what you meant by this. A list of high level stuff like 
text completion, code completion, image generation, sound generation, move 
generation doesn't seem helpful. Not sure if you wanted to know what the companies 
that sell AI-enhanced ed-tech are hawking to get a list of capabilities tailored to K-12? 
Maybe, but then ...
 
Classroom reality. Presumably this is where the K-12 specific stuff would go? Is this 
supposed to be how students are using now in FCCPS, anywhere in the world, or 
hypothetical vignettes that illustrate the opportunities and challenges of AI in the 
classroom?
 
Here's the outline as it stands now, though it will be difficult to get it all filled out by EOD 
tomorrow. Can we send something like where we're at tomorrow but then have a few 
more days to finish up?
 
On Friday, I assume we'll get some amount of time to talk about what we did and what 
we found. What should we plan for? 
 
Thanks!
Tom
 
 
 
 
On Mon, Feb 9, 2026 at 8:01?AM Tom Colvin <thomas.j.colvin@gmail.com> wrote:
Roger that!
 
On Sun, Feb 8, 2026 at 1:19?AM John Black <john.black@outlook.com> wrote:
Hi Tom,
Thank you again for leading Research & Shared Baseline, with Elizabeth as co-lead.
Your next deliverable 
Shared Baseline Brief v1 (2–3 pages) for committee review at Meeting 3 (Fri 20 Feb)
Definition of done:
*	AI capabilities/limits (plain language; practical implications for schools)
*	Misconceptions to “kill” (prioritized, with brief corrective framing)
*	Key terms / definitions (working definitions suitable for policy drafting)
*	“Classroom reality” examples (student use, teacher use, integrity triage)
*	Annotated resource list (top ~10)
Inputs available
*	Focus Block sticky-note extractions and dot-votes are on the Miro Shared 
Baseline frame.
*	You can also draw from the Team Formation materials, Resources area, and 
your own research.
How we’ll use this on 20 Feb
*	The Shared Baseline will feed our Values & Principles convergence (8–12 
principles + one tradeoff statement).
*	We are aiming for enough convergence by Meeting 4 (6 Mar) that all 
workstreams can draft in alignment.
Logistics
*	Please coordinate directly with Elizabeth on division of labor and synthesis.
*	If possible, please post (or email) v1 by Tue 17 Feb EOD so I can compile pre-
reads.
Suggested structure (if helpful)
1.	Capabilities/limits (what’s reliable vs brittle)
2.	Misconceptions (ranked, with correction)
3.	Working definitions (short)
4.	Classroom reality: 3–5 concrete vignettes
5.	Implications (what this means for policy design)
6.	Annotated resources (top 10)
Thanks again, 
John
 
John Black | +1-425-223-7804 | john.black@outlook.com
 
