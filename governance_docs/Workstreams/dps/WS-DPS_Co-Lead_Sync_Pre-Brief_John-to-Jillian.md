# WS-DPS Co-Lead Sync Pre-Brief (John → Jillian) — for today’s call

## 1) Desired outcomes for this 15–20 min sync
- **Align on v0.1 structure**: keep the 3 deliverables + readiness checklist as the spine.
- **Confirm/adjust the risk-tier model** so it works in real FCCPS usage and approval pathways.
- **Translate “policy” into workflow**: who does what (IT / procurement / curriculum / SPED / legal / building leaders), and what triggers escalation.
- **Select 3–5 committee “tradeoff questions”** to tee up for Meeting 4 (Thu 6 Mar).
- **Decide how to use AIEIP excerpts** on the Miro board (scope, attribution, permission).

---

## 2) Jillian’s v0.1 package — quick recap (what’s already strong)
**Deliverable 1 — Risk tiers (plain language + minimum requirements)**
- **Tier 1 (Low)**: no student-identifiable data; building/department approval; inventory entry.
- **Tier 2 (Moderate)**: anonymized/aggregated student data; DPA + retention; annual compliance review; IT + curriculum approval.
- **Tier 3 (High)**: identifiable student data (accounts/logins, SIS/LMS integration, IEP/504); security assessment, bias assessment where relevant, human-in-the-loop for consequential decisions; semi-annual audit; district-level approval (IT + curriculum + admin + legal).
- **SPED review layer**: heightened scrutiny regardless of tier; explicit consent + IDEA-informed protections when disability-related data involved.

**Deliverable 2 — Procurement/governance decision checklist**
- Step 1: classify tool (Tier 1/2/3 + SPED overlay)
- Step 2: verify “must be true” items before adoption (cross-tier non-negotiables + tier-specific checks)

**Deliverable 3 — Top risks + mitigations + suggested owners**
1) Unvetted tools handling student data  
2) IEP/504 data exposure  
3) Algorithmic bias in assessment/intervention  
4) Tool proliferation without oversight  
5) Vendor practice changes / downstream model-training risk  

**Readiness**: 3 guardrails + 3 tradeoffs + dependencies list.

---

## 3) My “value add” on the call (proposed refinements)
### A) Keep 3 tiers, add two *escalators* so Tier 1/2 doesn’t under-cover real risk
- **Escalator 1: Integration** → if tool connects to **SIS/LMS/SSO** (or any identity-linked system), treat as **Tier 3 review** (even if vendor claims “anonymized”).
- **Escalator 2: Consequence** → if tool influences **placement, grading, intervention, discipline, SPED decisions**, treat as **Tier 3 review** (high-impact use).

### B) Clarify Tier 1/2 boundaries around “student artifacts”
Student writing, images, audio, and project work can identify a student even without names. Suggest adding a Tier 1/2 note:
- **“No student identifiers AND no student-generated artifacts that could reasonably re-identify a student, unless explicitly approved.”**

### C) Make monitoring feasible (minimum viable, not perfect)
- Shift from “audit everything” to **a lightweight cadence**:
  - Tier 2: annual vendor attestation + spot checks
  - Tier 3: semi-annual review focused on **data handling + changes in terms + incident history**
- Define “what counts” as monitoring evidence (change notice, breach notice, data deletion confirmation, subgroup outcome review when applicable).

### D) Add 2 missing tradeoffs that will surface anyway
- **District-provided vs BYO tools** (teacher experimentation vs enforceability)
- **Data minimization vs instructional personalization** (value vs exposure)

---

## 4) FCCPS realities to confirm (high-leverage questions)
1) **Who owns tool approval today** (and for non-AI tools)? What’s the actual intake path?
2) Do we already have **DPA templates**, a security questionnaire, and a standard vendor process?
3) Where will the **AI tool inventory** live, and who maintains it?
4) What’s the current **records retention** posture for student-facing digital tools?
5) What’s the best “systems/data flow” *starting sketch* (SIS, LMS, SSO, assessment, SPED platforms)?

---

## 5) Using AIEIP excerpts without overloading the committee
**Why they help:** They supply *justification language* for guardrails (regulatory gaps, risk evidence, and core values like equity/transparency/accountability), plus an implementation lens (PD + governance + bias monitoring + stakeholder engagement).

**How I’d recommend positioning on the Miro board**
- Add as a **Resources** module with clear attribution + “excerpt” labeling.
- Link to the **full capstone** only if Jillian consents (and we’re comfortable with visibility).
- Translate DEIPAR into FCCPS-friendly language for committee-facing slides (e.g., “equity lens,” “multiple student identities,” “power/impact awareness”) while preserving the intent.

---

## 6) Proposed division of labor (lightweight + sustainable)
- **Jillian**: continue drafting tiers/checklist/risk table and committee-facing tradeoff prompts.
- **John**: pressure-test technical realism (integration + consequence escalators), convert dependencies into named owners/RACI placeholders, and draft a short “FCCPS workflow sketch” page.

**Weekly rhythm (suggestion):** 1 async checkpoint + 1 short live touchpoint until March 6.

---

## 7) Close-out question for today
“What are the **two decisions** we most need from the full committee on March 6—and what do we need from IT/procurement/legal *before* then to make those decisions real?”
