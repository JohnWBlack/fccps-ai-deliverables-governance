FCCPS AI Advisory Committee Meeting 3

Fri, Feb 20, 2026 8:30AM • 1:15:50 (Transcript starts at 08:38 – The first eight minutes followed the published meeting agenda).

## SUMMARY KEYWORDS

AI literacy, K-12 education, policy challenges, academic integrity, mental health risks, AI benefits, student use cases, teacher use cases, AI access, pedagogical practices, ethical AI use, community feedback, policy recommendations, trade-offs, educational goals.

## SPEAKERS

Elizabeth Chua, Bethany Henderson, John Black, Speaker 2, David Berol, Speaker 3, Speaker 4, Speaker 9, Speaker 7, Thomas Colvin, Speaker 6, Speaker 8, Speaker 5, Speaker 1

# SUMMARY

The meeting focused on the challenges and misconceptions of integrating AI in K-12 education. Key points included the uncertainty of AI benefits, potential risks like academic integrity and mental health issues, and the inevitable access to AI despite school policies. The discussion emphasized the need for clear principles and trade-offs, with examples like AI literacy and the importance of ethical AI use. The committee debated the importance of aligning AI with sound pedagogical practices and the need for ongoing review and adaptation of policies. The next steps involve refining principles, trade-offs, and preparing for community feedback and town hall meetings.

## Action Items

-   [ ] @John Black - Send another committee survey to collect feedback on principles and candidate items (notify members and solicit revisions)
-   [ ] @John Black - Merge workstream deliverables and produce a consolidated 'cut list' / merged draft for the committee to review and circulate it this week
-   [ ] @John Black - Receive and process written workstream inputs and materials so they can be merged for the next meeting (submit materials by next Friday to enable merging)
-   [ ] @John Black - Provide a method to collect public inputs for the upcoming town hall (define collection approach and ensure it is ready by the town-hall date)
-   [ ] Prepare and send written guidance (ground rules) for committee members about participating in community events and public meetings
-   [ ] Coordinate with the town-hall community input group about town-hall logistics and attend/participate for a few minutes to align messaging and process
-   [ ] Collect missing AI use cases and supporting evidence into the evolving document (request committee members send any use cases or evidence they have)

# OUTLINE

## Challenges in AI Policy Development

-   Thomas Colvin discusses the initial assumptions and misconceptions about AI in K-12 education, highlighting the challenges in creating a strategy.
-   The benefits of AI in education are uncertain, and there are concerns about AI replacing teachers and impeding learning.
-   AI introduces new risks, such as academic integrity issues and mental health concerns.
-   Students and educators will have access to AI regardless of school policy, making it difficult to control its use.

## Defining AI Literacy and Classroom Examples

-   Thomas Colvin explains the definitions of AI, including AI language models and large language models.
-   Elizabeth Chua adds insights on AI literacy, emphasizing its comprehensive nature.
-   Classroom examples are provided to illustrate both the benefits and tensions of using AI in education.
-   The discussion includes considerations about parental permission and the potential for AI to save teacher time.

## Addressing Misconceptions and Assumptions

-   Thomas Colvin emphasizes the need to address misconceptions and assumptions about AI in education.
-   The conversation touches on the empirical nature of AI's reliability and the importance of actual experience and research.
-   Thomas Colvin encourages participants to provide use cases and evidence to support or refute the use of AI in K-12 education.
-   The discussion includes the importance of connecting recommendations to the problems they address.

**Virginia Level Policy and Grade Bands**

-   Thomas Colvin mentions the Virginia level policy, including an executive order and a 100-page report on AI in K-12 education.
-   The report suggests conservative prototype projects and emphasizes the importance of gathering community feedback.
-   Elizabeth Chua discusses the controversy over grade bands and the structural reasons for mapping schools by grade levels.
-   The conversation highlights the need to align AI use with learning goals and IB values.

## Ongoing Work and Reducing Uncertainty

-   John Black thanks Thomas and Elizabeth for their work and emphasizes the ongoing nature of the research.
-   The goal is to reduce uncertainty enough to create useful and flexible policy.
-   The discussion includes the importance of connecting AI use to learning goals and the need for empirical evidence.
-   John Black suggests that the committee should agree on problems rather than solutions.

## Principles and Trade-offs in AI Policy

-   John Black introduces the process of developing principles and trade-offs for AI policy.
-   The conversation includes the importance of human sponsorship for each principle.
-   Participants discuss the need for clear trade-offs and the potential for AI to enhance learning theories.
-   The discussion touches on the challenges of measuring the effectiveness of AI in education.

## Balancing AI Use with Pedagogical Practices

-   The conversation includes the importance of balancing AI use with sound pedagogical practices.
-   Elizabeth Chua raises concerns about AI undermining teacher feedback and the student-teacher relationship.
-   The discussion includes the need for AI to support cost-effective implementation of effective learning theories.
-   The conversation highlights the importance of evidence-informed decisions and the role of experienced educators.

## Core Philosophies and Policy Guidance

-   Bethany Henderson emphasizes the importance of core foundational philosophies in guiding AI policy.
-   The conversation includes the need for periodic review of AI policies to ensure they remain relevant.
-   John Black suggests that the committee should aim for a balance between generality and specificity in policy guidance.
-   The discussion touches on the importance of clear communication and the role of the school board in oversight.

## Next Steps and Community Engagement

-   The conversation includes the need for clear guidance on community engagement and town hall meetings.
-   Participants discuss the importance of gathering input from the public and the role of the committee in these events.
-   The discussion includes the need for written products from work streams to inform policy formation.
-   The conversation highlights the importance of ongoing research and iteration in developing effective AI policies.

# TRANSCRIPT

<https://otter.ai/u/TpP86O53zA1Ge6a2m8kSunoedC4?view=summary>

{First eight minutes not captured on transcript but followed the published agenda: Adoption of the agenda, welcome and introductions, adoption of minutes from the previous meeting, public comments, review of School Board Charge, and School Board Liaison Report, in which Bethany Henderson announced the upcoming School Board AI town hall meetings on Monday, March 2 @ 6:30 PM at the Learning Stairs at Meridian High School, and on Saturday, April 18 @ 4:30 PM at the Community Center (Senior Center), at which the AI Advisory Committee is invited to participate).

**Thomas Colvin** 00:00

Through a little bit of what's in here, where we left off and where we are now, and hopefully everyone will so just at a high level, when we left off last time, we had a bunch of assumptions that we were trying to pull together. We have misconceptions that need to be addressed and reframed. And we also had a number of different student and teacher use cases, some of which we said, Oh, these are allowed, and some of which we said, Oh, these are not allowed. To pull those things together, though they often tended to cross boundaries. All of the assumptions to design for were things that may or may not be true, right? The misconceptions, again, were things that may or may not be true. And so tried to layer these into some of them were the purpose of why we're here. Like, what is it that we are trying to do? Many of these things wound up being principles for how we develop policies. So, like, as we try to do this thing, what are we trying to do? Or how do we try to do it? But most of them wound up being basically challenges. And so these are the things that make creating such a strategy difficult. So in the document, you can see where I layered all we layered all of the different things that you all submitted through stickies, throughout those different things, the thing that I think stands out the most are that there are a lot of challenges associated especially with that we don't quite understand what the benefits of AI in K through 12 education may be. We have misconceptions like AI may replace teachers, or AI hurts or impedes learning. We can't necessarily reframe those in a positive way, because it may not be true, but it also may be true, right? It depends on the specific instantiation in our school districts, whether or not these things turn out to be true. So these are challenges that have to be addressed as we create the policy so you can go through the different challenges, right? Like the benefits of AI may be uncertain. The use of AI introduces new risks. We've talked a lot about academic integrity, but less about many of the other risks, such as students may be less likely to reach out for mental health for a person, or they might generate media that is, we'll say, offensive and representative of their friends or colleagues or teachers. We also have students and educators will have access to AI regardless of school policy. That's a challenge, and it is generally not possible to tell when someone has used AI. So encourage you all to look at the challenges and keep them in mind. Editorializing a little bit, having worked on many, many national and agency level policies, one of the ways in which I see these efforts fail is that people give what they think are good ideas, but don't clearly connect them to the things that make the problem difficult, and then in the end, you have a list of recommendations that are not clearly attached to problems, and it becomes difficult to argue for the recommendations or to make changes because you haven't fully closed that value proposition. We provide definitions, current definitions, about, you know, like the state of AI. We started AI language models, large language models, Gen AI, et cetera. The focus of those is to show that these are ultimately statistical tools and that they are not necessary. It's basically just math, right? And it's not this magic thing that's always going to give you the right answer. Elizabeth is a schooled me on what AI literacy means. I did not realize just everything that that encompassed and so she has added a number of things that will hopefully help get us all on the same page about what we mean by AI literacy. We have some classroom examples. Encourage you to look at them. We tried to make them in such a way that they not only show here's what we could do with them and how be beneficial, but also illustrate some of the tensions. Right? Like as you let a teacher do grading of materials, great. It saves them time. But did they get permission from the parents who may not have wanted their students information uploaded to a, you know, a system like this. We try to give those considerations as well. And then, you know, finally, when we talk about whether things are misconceptions or assumptions or whichever. Everything may be true. Everything may not be true. We don't know. And these are ultimately like, it's premature or oversimplified to say that certain functions or uses of AI are reliable or they're brittle, or they're good, or they're bad or whatever. These are ultimately empirical questions where we should be able to look at actual experience from school systems or the best available research to say something like using AI for brainstorming has been shown in these areas to be good, and there's evidence here that shows that it's bad, right? So we've tried to start a list, and this is where it would be great to have your help of different functions that AI may provide. So like it may help you brainstorm topics, it might help you with your grammar, it might help you with your style, it might help you personalize learning with algebra or something. So we've got these different use cases in a matrix, and then we're trying to connect that to research or other evidence that either supports that as a use case that is valuable in K through 12 education, or that undermines the notion that it will create positive learning outcomes. So hopefully, you know, as we evolve this document over the course of the next few weeks, you could send us any use cases that you think are missing, and also any evidence that you may have come across that supports or refutes these types of things. They've tried to do that. Thank you, but I'm sorry, like thus far, I've I've had my head little underwater with work lately, so, but we hope to evolve this as it goes forward. There's also a section on Virginia level policy. So, you know, there was an executive order, there was guidance that came with it. Also, a couple months ago, they released this like 100 page report that gives detailed guidance about, you know, implementation at different levels for this group. The Bluff is basically that for use in K through 12 education, the Summit and the guidance basically said, in K through 12 education, it's, we're at a very early stage of maturity. These should be conservative prototype projects. We're not at the point where we can, sort of like, go full steam ahead with this stuff. And they suggested a very interesting way of gathering feedback from the community, and stressed how important it is to get rapid, frequent, like easy feedback from everybody, students, parents, educators, school board members, concerned members of the community. And they lay out a process for doing that, which I thought was interesting. So sorry. I feel like that's basically it for now, for me, I don't know a little bit. I guess the things

**Elizabeth Chua** 07:30

that I would add is that, as we practice, we need to make sure that we don't just focus on AI in itself, but focus on what we want students to learn, and keep in mind the sort of IB values, assume we're all on the same page with that, and make sure that any AI use is in the service of achieving those kinds of learning goals and making sure that that's consistent. I think that's a good place to start. And then the other charge was about grade bands and looking at the stickies. There was some controversy about that. It seemed like it fell into two camps of like one mapping on to exactly how our schools are set up, K through two, three through five, and then some just lumping elementary schools together. My personal feeling is that we should match the schools. I think that there's good reasons for that, structurally, but also, if you look at existing AI that are out there, they do follow K through two, three through five. And so it's nice to map on there. There's certain learning goals that they put their age appropriate for K through two. And so that structure already exist and in national high space, so it seems good to pursue that rather than lumping elementary together. And in my opinion,

**John Black** 08:52

so I want to thank Tom and Elizabeth for all this great work. It's an on as you, as you've alluded to. It's an ongoing work research for our committee. It supports all of our work streams. So all the work streams are where you're going to find the application, the use cases for the foundation that you've laid you it's sort of, it's extra challenging for you because, you know, I think I talked about in the first meeting that we are on a journey to reduce uncertainty and how far we can get, how much uncertainty we can reduce, of what to do and how to do it. Those two axes, it sort of defines whether or not we'll be successful. We're not going to get to certainty. It's just not going to happen. I think that this is completely clear after after your presentation. But can we get to enough certainty to create policy that is useful, enough, relevant, enough, flexible enough to be useful? And that's that's the charge. So considering that this is to be an ongoing work, supporting all of your work streams and a non binding statement, can we agree to disagree or agree to that these are the problems, if not the solutions? Yes. In the

**Speaker 1** 10:10

first baseline assumption says that access is inevitable. Is that an assumption that we all agree with? Because, going to your point of looking at our schools, is access inevitable for our youngest students.

**John Black** 10:26

So I don't think it's qualified by inevitable that it's going to be in school. I think

10:33

I would say this, AI, is not as bad that's going to be gone in like next year. It's how I would view that statement.

**Elizabeth Chua** 10:44

And I think part of the charge here is to figure out what kind of access might be appropriate for grade levels. So some schools, for instance, say that in K through two, access is for teachers, not for students. I can tell you, I asked my son, he's second grader this morning, like, have you heard about AI? What have you learned? And he's like, yeah. He's like, what did you learn? He's like, AI is helpful. And I was like, Oh, what did you learn? Or where did you learn? That is, like, in tech class, we did hour of AI. And I was like, what did you learn? And he's like, I don't know, yeah. So they are getting exposure in school, even in our younger students. But he said, he said there were no demonstrations which, like, I don't have a run for Lyle the narrator situation, but

**Thomas Colvin** 11:28

like, kind of following on with that like this is why, at least in the larger document, you know, we Don't use language like this, because access is inevitable, is an assumption that depends on somebody that students and educators will have access to AI regardless of school policy. So like indeed, maybe some kindergartners have it, maybe some don't, but that school policy, the thing over which we have control, doesn't determine their access to it, right? So it's like an external factor. So I have just me personally. I mean, like, you know, of course, like, we'll move through this. I have a bit of a personal issue with trying to say that these are assumptions that we designed for. These are challenges I think, that we must address that we don't control their access. That right now, detection may be weak. Detection may be weak in some areas, not in others. This may change, right? But so like these are things that we just don't know more than their assumptions that

**John Black** 12:35

we should make. I would rephrase it as a negation of absolutes. The absolutes don't hold up, so we have to consider the alternative, right? And if the absolute is, you can control everyone's access to AI. That might actually not be possible. Probably not,

**David Berol** 12:55

sir, I'll just say, in general, I agree with everything on this. Ai generated summary, and we could spend the whole meeting, like, on like, word level issues. So I'm just trying to figure out how to move forward. And also, I haven't read, like, your full document sounds fascinating, but I haven't read it. And so maybe the path forward here is just to agree that like, like this summary raises the right issues, and we should keep thinking about them, because I I could spend 10 minutes talking about like, little things that I would quibble with, but I don't that's relevant for right now.

**John Black** 13:36

I think that's that's a great observation, and I would like to second your motion, this work raises great issues that are relevant to our work can and we agree all those in favor, aye, opposed. Okay, so that means that we're going to continue to study and we can move on, all right, so the next part, oh, I'm sorry. Sorry, is is our focus block on values and assumptions.

**John Black** 14:18

All right, if you have perused the Miro board this week, or last, you would have seen this flow of process and a little bit of explanation of how it works. That is, that starts with themes that were derived from your inputs of three risks and three opportunities meant to be, you know, not to be debated. It's just a it's there. These are, these are six umbrellas that we can fit all of our principles in. If there was something that that couldn't fit, then yes, all right. So what we need to do is get to principles and trade offs right? For every principle, there's a trade off, because the principles are informed by values, and values come into conflict, and when, when push comes to shove, we need to know which to emphasize one or the other. So it starts with the the inputs, and then principal candidates. I have 12 seeds generated by AI. You're asked to pick two, to sponsor and write one, but that's, you know, that's and I know that some of you have some things in your hip pocket. Get ready to write. Maybe you're already writing on your sticky notes, because you're going to be putting them on the wall, which is a representation, the same thing that that is on the mural. Board is represented in the physical world. The ground rules for the board are humans, all right, so AI is helping us, right? Humans move the cards. Every principle that we decide to pass, revise or park gets a human sponsor, at least one. You can have more than one, and you signify that by putting your initials on the card. Right? You can write your card as well, and if you write one, then sponsor your own right. Does that make sense?

**Thomas Colvin** 16:28

I hope you'll get there. But like, I am interested to know what a principle is. Usually in principle, like in policy documents, I'm aware of principles are you've already said what it is that you're trying to do, and principles are how you will do

**John Black** 16:45

that, like the value, no, no, this. We are not in the how this is.

**Thomas Colvin** 16:49

We are in the why. I'm asking, what is a right? So this is a

**John Black** 16:53

principle template we will do this or prioritize X to achieve y, so that z. And here are 12 examples which you could accept, revise, reject, Park. You can do anything you want with, right? But they're fit within these six themes, all right? And ideally, and really, we must have straight a trade off statement for each principle. I don't know if we'll get there today, but that then informs the how, right, how the application of the principle is through trade offs, right, control, access, right? The competing values framework, create, compete, control, collaborate, those things are going to come into conflict when we try to apply our principles into policy and into action. So, okay, we'll use AI to help synthesize. Because what we want to do is we're going to we're going to diverge, and then we got to converge. We want to converge on eight to 12 principles. You could, you could, you could all sponsor these and decide to pass these that they meet the criteria, and we can be done. But I suspect we don't want to do that. We want to we want to think about it. As you go up to the mural board, you're going to place your candidates in the under the theme where it belongs.

18:37

You're going to initial it

**John Black** 18:43

and and then when we when we doing that for a few minutes, then we'll talk about applying the quality,

18:53

the quality filter to them,

**John Black** 18:58

if we think they pass the quality, if someone, if a sponsor, thinks it passes the quality filter, we'll put it in the pass, in the Pass area. If we think that it has failed one of the quality filters, one or more, put it in revise. If we think that we don't want to revise it, we just want to park it for whatever reason, put it in park. And then when we think we have revised what we want to revise, then we'll try to get it. We'll have maybe 20 things right that we need to get to eight to 12 things. We'll ask AI to help us collate them. Or, if humans want to, want to jump ahead and do that. Do it right now. I ran this as an AI simulation with AI playing AI and AI playing us. Actually had personas,

**Speaker 2** 19:53

but that were not us, right? They were but they were different roles, where

**John Black** 20:02

those green ones, the yellow one are, are the write ins, right that you're writing, and then the quality filter was applied to To these was that 20, I don't know how

20:24

many. Sometimes stuff gets

**John Black** 20:27

straggled, apply the quality filter, and they fell into these buckets, pass and revise. A revision exercise. This is where we're going to, again, run into time, because we've got 20. Got 20 minutes. All right, is writing the trade off statements. All right, so we may have to take this offline between the meetings to get to trade off statements. All right, so go ahead and go to Wall, please, and and let's see some more candidate principles in these things,

21:04

or we could vote on the ones that are there.

**John Black** 21:07

Well, let's see. Let's, let's allow people to ideate first, but certainly we are going to vote. And if, I mean, if you like, if you think, hey, that meets the quality test, then put your and you think you're going to sponsor it, right? I'm not gonna let AI run, run our decisions. I want humans to to, you know, sponsor that Sure. Yes, I'll

**David Berol** 21:32

just say I'm not really good at thinking fast. Okay, so I wrote my thoughts on double sided piece of paper that I can stick to the wall, but I can't come up with thoughtful ideas in a 32nd timeframe for AI to then, like, further grind down on, yeah, so say this is what I got, this page,

**John Black** 21:56

okay, um, can really, I can't, like, tear The Back off, but, can you, for every one of your paragraphs or your nuggets of what you have, can you write just a few words on a sticky note and stick it on one of those regions so that I know something's there? I can

**David Berol** 22:16

try and put like a placeholder, but I put some thought into what I wrote, and so I can't Yes,

22:21

yes, you always do, and that's good to the way. Oh yeah, yeah, this is the simulation. I'm sorry.

**John Black** 22:39

Don't you want the themes or The candidates, these are The themes Do.

**John Black** 23:22

I That's how we can see that example, right? Oh no, no. Actually, we're on this region on the left. We're not okay, we're not yet at revised yet.

**John Black** 24:08

So the suggestion that each of you just write one sticking out right, and you could, and it could be a completely new or could be a revision of of of one of the 12 seeds that's that's up there, do we need to put like passion points for others if we also agree with them? So if you're going to get a chance and to do that when we when we determine if it's pass revised but, but I just want to see at least one person wants to adopt each one? Well, any of the 12, you can just leave them behind as well. If nobody sponsors them, then we'll leave them behind and and work with something else. I

**John Black** 25:14

So David, you had about, I don't know, 812, things, yeah, that you shared, and and I'm only asking you for one of them, but if you want to write more and

**David Berol** 25:27

let people look at it, Well, there you go. That's what overachiever.

25:41

It's 904.

**John Black** 25:47

Adriana, the step ditch revised, where that? Where did it come from me? I know, but a revision of what so that was from

25:58

the deep skills first, then Put it over there, please. Yeah, progress.

26:20

We have here. If we wanted to move

**John Black** 26:39

one of these two, just move it over. So we're going to do that the next step. Okay, yeah. So right now you can, yeah, initial it, if you want it, if you want to adopt one of the things, it's, we're not saying that it passes yet, right? So if you think, huh, that's, that's, it's interesting, but it needs revision, go ahead and sponsor it, because then, then you're going to move to put it in revise got it

**John Black** 27:20

Are there some that are up there that aren't educated? No, these 12 are there On the white stickiness of the

**Speaker 3** 28:40

I love it You trade off. I Okay,

**John Black** 29:39

so if you put something up on a sticky, put your initials on it so that now that you're its sponsor

**David Berol** 29:46

on exactly one that we sponsor. Is that the idea?

**John Black** 29:49

Well, if you write something, if you write a few, every sticky that gets put up there needs a sponsor. And if we are to use the AI Seeds, then they need human sponsors as well. I

30:30

We can endorse other people's post its right.

30:33

You can, if they didn't want to endorse their own. I

**Speaker 2** 31:01

We have the same initials. It's okay. It's all good. What's your middle name? Mary,

**Speaker 4** 31:17

my mother's middle name, my grandparents. Just that

31:41

beautiful. So you did it.

31:59

Okay, Okay,

**Speaker 5** 32:18

so, why Don't

32:40

on Thursday,

33:10

Curious? Anybody?

33:40

Thank you very much.

**Speaker 2** 34:18

I'm using lens which straightens them out.

34:27

I have to do it now, because I'm gonna process it now. Do

**Speaker 2** 35:02

I can't read it.

**Speaker 4** 35:22

Do? It's made up. Population, yeah, I need to get that. I already got that, something in my corner, put it back once I do this. Okay,

35:59

if this works, do

**Speaker 4** 36:20

public safety or energy that is putting on something when You we can have synthesized a lot of people.

37:02

Not prompt apply.

**Speaker 6** 37:21

Quality filter two, but like once a year, choose

37:58

and propose to meta, pass revise Park. We use Extended Thinking

38:17

that's going to take A couple of minutes so

**Speaker 4** 38:34

and seeing the difference. About

**Speaker 2** 38:47

so we're applying quality filter drafting

**John Black** 38:56

right as well as all right. So right now, I'm taking the seeds and your write ins, running them through a prompt, apply the f3 quality filter to the principal candidate, seeds, and write ins and propose to pass revise or park. It's going to come up with a proposition, and then we have to determine if we agree with it or if we don't. And then we either decide to park it or continue to revise but the reason you know why the reasons why a particular principle might not pass, is it doesn't have it doesn't get us to action, doesn't it doesn't guide decisions. If it's impossible to measure anything against it, if it's if it doesn't, it has to at least point to a specific implementation, not a hypothetical, and there are trade offs, but it has to be, has to be the trade off is actually separate from the principle itself.

40:18

So I told it to think a good long time

**John Black** 40:25

about that. But anybody want to talk about what you might what you've put into these things? David, I know you put eight of them in there, so I'm sure you have some something you might like to share.

**David Berol** 40:39

Sure mentioned one. I mention one on AI literacy. I think that's easy to misconstrue as like, Oh my God, my kids are going to need to use AI. So I want the school to have them use as much AI as possible so they'll be prepared for the workplace. And I think it's important to frame AI literacy as being just as much about knowing when not to use AI, so that people don't see a false trade off between restricting AI usage and preparing students for a world in which AI is ubiquitous. Because I think those are complementary objectives. That's just one, one item

**John Black** 41:20

that I would you had said early on that the term AI literacy is a bit of a pet peeve or a bugaboo to you.

**David Berol** 41:28

Yeah, I just think it's hard to know exactly what it means. And I think there is one construction of AI literacy, which is sort of like, let's you know, hit the accelerator pedal the max in order to prepare our children for a technologically sophisticated future, and I think that would be a mistake.

**John Black** 41:44

Do you think that was the approach to computer literacy? I do. And do you think that that had negative there

**David Berol** 41:51

are baked in problems that are hard to unwind in our current educational system as a result of decisions made 20 years ago to integrate laptops to the max. I'm not that's outside the scope of today, but I'm just saying, but it's instructive. It is instructive. You know, you see everything, David, I feel like what you're saying here dovetails with some of what I wrote in the one thing I stuck up there was essentially along the lines of, don't necessarily require students to use AI in order to achieve academic excellence, which I feel is a difficult thing to test, but essentially refining some of these principles together, there may be a possibility to create a policy that will instruct people properly on the use of AI, and educate them about AI, and then leave the choice up to them in terms of how much they integrate it right? And I agree with not requiring students to use AI. And there is also a way in which people can be required to use AI, even if the teachers don't say so, if there's not, if the assessments aren't structured properly, then essentially you are required by competitive pressure, to use AI Correct. Yeah, like, we dump so much stuff on our students, such that they essentially have to respond by using AI to accommodate for that, for instance, rather than really just getting them to think critically about a more limited amount of things, right? Or you have a race to the bottom scenario where you just know all your peers are using it, and you'll be judged the kinds of standard that's not achievable without the use of AI.

**John Black** 43:36

Did you happen to read, or, I'm sorry, watch the panel discussion that accompanied the Brookings Report, and the student, one of the people on the panel was a student, and that's what she said. He said, Everybody's got to have a four, oh, everyone's got to have a four, oh, to be competitive, and therefore it's an arms race, right? A prisoner's dilemma. How can they not use it? And that that is that seems like a scary future. Seems like it's actually a scary present. Are the

**David Berol** 44:10

students at Meridian graduate with at least a four? Oh, could be disclosing that, but I think it's or above a four, zero, it's a rather high percentage.

**Elizabeth Chua** 44:19

This is a larger issue of sort of, like, AI is really good at the product, yeah. And so there's pressure to produce, like, students are feeling that pressure to, like, produce the product. And so there's this temptation to use it. And so I think we

**Speaker 1** 44:35

really do want to keep the focus on the learning. And that's

**Elizabeth Chua** 44:39

where a lot of the postings are sort of trending, but I think I tried to write this in a post. It with teacher preparation, like very badly. But one of the things that I'm also concerned about is as students are using AI as a tutor or have an AI polish up their writing, that teachers are losing that feedback information about where students are really at and so we're also undermining the ability of teachers because they don't know where students are in the learning, because all they're getting is this product, right? So it's the same issue. If the student does a draft and then the parent, like, finishes it, you don't want the teacher judging the parents work right. That doesn't help the teacher know what the student needs to learn. And so I think that there's also parts about the student teacher relationship that we need to keep in mind as we

**John Black** 45:30

craft these usage policies that goes back to like a fundamental goal of education, right? And our desire to have the students be drivers and not passengers in their education, and that seems to be a very tough thing. It's always been tough to swing and what in the age where answers are free, now the competition that goes who can pose the better question, and so those with curiosity and agency are going to, are going to excel. They're going to go faster with AI, but those without and want to use AI as maybe to do the thinking for them, are going to fall farther behind.

**Thomas Colvin** 46:14

Just completely agree and flip stopping everything. Elizabeth said, yeah. I also want to just like question one assumption that is within this, which is that AI is good at, like, making the product, or that, you know, kids are going to go faster, and somehow faster is better. Like, there was a recent study just came out. In the last couple of weeks, National Bureau of Economic Research, they interviewed CEOs and CFOs from 6000 different companies across the United States, Germany, Australia, one other country I don't remember, and fully 80% of them said they have not, despite making huge investments, have not seen any effect to their workforce, because they Haven't seen any effect to their actual operations. They have got no operational improvement. And when asked, What do you think about the future? They say, Oh, we think we're going to get, we're going to get benefits in the near future. How much? About 1% so it's not clear that we are in a spot where AI will actually like, it'll crank out a product. But we're not in a position where we can say that it will actually be like, good or even create the product. Well, it might maximize clicks on your laptop, but it won't.

**John Black** 47:32

You can't get you cannot solve the problems with the thinking that got you into them. Did you read the printer principles from the 60s, right? When it got to computerization and automation, it was the automation stupidity. So what we're what they're finding in these corporate is that their workforce is not engaged. They just show up at work. They sit in a queue. Somebody tells them to do something, they do it, and an AI doesn't really help

**Thomas Colvin** 48:06

with that. And so while I can fully agree with that, and we've seen that in previous ways, with AI hype, where it delivers benefits, trying to move things to the cloud, where people don't actually build things to be cloud native, they just drag and drop, and it doesn't work. The data point before, there are other incentives that are in place that may make it such that when we try to use AI here in a K through 12 system, we may also not see any benefits we could right, but we we may not. So I just want to encourage folks to realize that right now, most use cases don't appear to be seeing the benefits that the hype would see this fencing I'm talking about.

**Speaker 7** 48:53

Actually, the idea is that there are different errors that the AI was making, that the student is making, and the teacher AI product than the student product is not mismatched. It was problematic

**David Berol** 49:07

for the student teacher relationship, school related work product is exactly the kind of work product that AI is good at maximizing on because it's graded according to a rubric, and so I do worry that it will produce effortless essays that get seven out of sevens on the IB grading rubric, even though, like in the real world, it would fail, it might fail to produce efficiency within the conservation contrived assessment framework we have to deal with, because this is education. It is, I think, likely to produce

49:51

that jury work product.

**John Black** 49:52

So a meeting or two ago, we talked about pedagogy and how current pedagogy is not adequate in the age of AI and all the measurement of achievement is probably not adequate, and yet, what you just said is, I can't change it, right? You can't just change the IB criteria. And so now, how do you reconcile that mismatch

**Speaker 5** 50:18

about pedagogy? I think some of these risks, I agree with many of the risks, I do think it's worth acknowledging, though, that there are some real potential benefits here, which is that AI technologies can allow us to implement in a much more cost effective way, theories of learning that we already know are effective. So an example of that is like immersion in learning, you know, foreign language, we know this has been effective. There's lots of evidence to support it. It's a very costly thing to implement in this whole system. AI technology can make that much more cost effective. There are other kind of, I think, running rubrics that we have very good evidence for, but they're not so easily implementable without the use of some of these technologies. And so I think we just have to keep those potential benefits in mind. So when the technology is coupled to sound learning theory. So if we

**John Black** 51:06

have there are soft balls that are thrown to us, we should hit them. Or if there are slam dunks, we should take them and then concentrate on the things that are the messy middle and determine the we keep AI away from that, right? Or more, because what you're saying is you're able to, if something's an easy decision, we'll make that right, and then we can concentrate our effort on controlling the things, or trying to regulate the things that

**Speaker 5** 51:33

I think another problem this gets to one of the principles that was raised in the earlier document. You know, as much as we would like to be evidence driven and evidence informed about the effectiveness of a lot of these technologies, the reality is that the technologies are developing a base at which we're not going to have good studies that confirm or deny the effectiveness, I mean, in any kind of reasonable way, the effectiveness a lot of these things. And so it would be nice to be able to always point to a good, conclusive study of that kind, but in many cases, we're going to have to trust to the discretion of experienced educators about the pedagogical value of this to I don't know, more anecdotal feedback from the relevant parties, from teachers and students and stakeholders. That's just one of the challenges I think we're going to face. We're not face. We're not going to have great studies

52:24

here. So in leaks preceding the 1991

**John Black** 52:31

invasion on the border of Iraq and escape shall kill progesterone. What?

**Speaker 3** 52:40

Be or Not. I think something you

**Bethany Henderson** 52:53

said is really important, and I don't need to get lost, because at the end of the day, your brain, Dr Bates and his team are going to figure out, you know what actually the rubber hits the road would happen. But the concept of tying acceptable AI used to sound educational, pedagogical, policy or approaches, excuse me, that is like a baseline we may not be able to define actually what either of those things are. And I don't think it's actually that helpful for policy to define what either of those things are, because it's moving so fast, right? It's not clear, but having that as a baseline for how the district approaches AI, that is the type of baseline that is super helpful

**John Black** 53:35

in policy. I guess we're looking for what cues we can follow, right? They were coming from academia, from our school system, from our government. And, yeah, I think there was some trepidation about we being given too much power here. And I don't think so, because we're not alone in doing this. There's a compass study. You're the school board member and and so, you know, we're what we are. We are present the public. We're not elected. We have no contractual obligation, and have enough citizens to do the best we can and to get the widest input we can from the public and from our own research. And although many times, you know, I don't feel like it, we're all experts in some way, in some way, so that's to kind of set your mind at ease that you know, we're going to do the best we can, and it's very likely we will mess this up, in some ways, highly likely. But if the school board and the school system is committed to learning and iteration,

54:52

then this is a baseline,

**Bethany Henderson** 54:55

and that core foundational philosophies are more important. All of this is important. What we're doing, anchoring it in that core foundational philosophy, which could be, you know, we want our kids to be masters of AI, but that is a core foundational philosophy. We want AI in the district use when it aligns with sound pedagogical practices. That is a core philosophy. And so I, you know, as you think about the ultimate work product for this committee. All of these things, as you said, need to shake an eye to the core philosophies that y'all are proposing should drive the district. And those are going to be, by necessity, non specific, but they need to be clear.

**John Black** 55:42

Do well, as you can see, we have some proposals for past revival Park, and we are out of time, and we have some good discussion going. I don't think we weren't really allowed to take the meeting further even, and people do have to get to places, I'm sure so I wonder. We can continue to discuss for a few minutes, but feel about that. How do you feel about that? Is anybody really burning right now? What time? Members, do you have any objection and discussion? Tease for a few minutes. We're capturing everything. It's not we're not going to get to a decision on pass revive park here today. So I'm going to be sending something out another survey.

56:40

I do not have objections, objections, right?

**John Black** 56:47

Yes, and yes. And also, we're also working. I don't know you see this here, but we're working here now, provided working for work streams. And each work stream has a lead and CO lead. And of course, I can't meet with them, but they are communicating with me, and there's good progress going on there. And I hope that these discussions are going to filter right back out into those work streams and come back to us. But thank you, Jillian, thank you very much. And I think we'll just stay long enough for Adriana to and I've got to stay long enough for it to capture where the initial cut is on pass revised Park.

**John Black** 57:26

But we can, you know, continue to discuss a little bit.

**Speaker 1** 57:31

So go ahead, I have one question just about the community work stream. We are basing our messaging framework on the the the value pillar was the words coming out of this meeting.

**John Black** 57:49

So we're basing, yes, basing on the principles, right? And the principles are informed by our values that conflict, and we express that in terms of when push comes to shove, we default to this unless, yeah, I guess

**Speaker 1** 58:07

I was just wondering, from a logistics standpoint, when will we convene, recognizing that that's a challenge, right, in order to move this to a final state so that It can impact because we're presenting at the next so,

58:26

yeah. So what

**John Black** 58:29

we Yeah between the two meetings is when I expect that we will get to firm up our principles and trade updates and so for you know, when I simulated it, each one of the principles once passed, then spawn for three trade off three options. Go with what. We prioritize a over b. We prioritize b over a, we do something different, right? And so we need to know where we where we sit on that right, and then with that, then you'll have enough of what you need to do the next step. Never going to feel like we have everything we need. And that's so that was going back to my training as an innovation coach. If you doesn't feel comfortable, you're probably doing it right. Get comfortable feeling uncomfortable because we are intentionally colliding conflicting values against one another, which creates discomfort. We we are having a series of divergent and converging conversations. At some point, like on six April, we have to get converged enough to make policy recommendations. There's going to be all these little side bar, you know, journeys on, and so everybody feels like, oh my gosh, we're going backwards, not forward. And I was assured by my coaches, when I learned how to do this kind of facilitation, that no, it's actually progress. If we just had forward progress at every moment, then we would the only way you do that is by laying down and waiting everything and just accepting everything that I say, or a dominant voice says, learned that AI says, and none of that works. So it's the conflict that actually makes this

**David Berol** 1:00:30

work in terms of end product. Think it's instructive to read Arlington's guidance and decide what you think about that guidance personally. Okay, because personally, I find the level of generality of that guidance to be unhelpful, because one of the problems with excessive generality is you end up with statements that are unarguable and sort of vacuously true, like teachers should use AI in a manner that's ethical, like, Oh, okay. News to me.

**John Black** 1:01:03

That triggers failure. Reason, I think number two,

**David Berol** 1:01:11

yeah, so I'm just cognizant of that as a risk. Well, you

**John Black** 1:01:15

know, if we push to it's not decision guidance, it's not it's not it's not helpful. It's not going to help educators, staff or students make decisions. I sort

**David Berol** 1:01:29

of ask myself, could somebody conceivably argue against the thing that I'm saying, and if so, that might be a reason why it's specific enough. If someone could have an opposing

1:01:40

viewpoint. I think that sums up your

**John Black** 1:01:42

pain right in the everything that we thrown out as a definition, assumption, misconception, is like, Oh, wait, the opposite could be true, absolutely, the negation of absolutes and trying to find some something that is helpful guidance in the mess and metal,

**Thomas Colvin** 1:02:05

I don't know if this will come up, but related to that concern about being maybe like too vague to be implemented, another way I've seen policies go wrong is that even if you try to get a little bit more specific, the people who wrote the policy are generally not present for all of the nitty gritty associated with implementing the policy, and the people who are doing the implementing will interpret what was said differently. And so you've got to try real hard also to make sure that you've defined all your terms and ideally, hopefully, the two of you will be there to be that bridge, so that way that meaning can be conveyed, but without an actual person to be that conceptual go between like, it's almost like the implementers receive a completely different documents than the drafters, having people who were there for like the whole that's the only thing I've seen this work.

**Bethany Henderson** 1:03:09

I would say there's a couple of things. One is making the philosophy here, in addition to the specific rounding it in explicitly, as opposed to just here are the 10 ways to use it or not use it. That's number one. Number two, you're right, is that people. Number three is the school board's oversight function and role. So we create policies, and that is on policy on Tuesday, right? Part of this policy, I would encourage, I think we will want to have in it is some sort of periodic review of how it's working. If that isn't built in, then essentially staff come back and say, here's what's working. Here's what's not in school, we can say, but we said this, or the policy says that that allows for that continual evolution and dialog. That's the reason we put that on the cell phone policy, because we know that's a rapidly changing environment, this similarly, needs to have some sort of not so frequent that it's ridiculous, but episodic is often enough that it doesn't run away. Because ultimately, you know, this will last Outlast Jared and I on zero for decades. Is not, you know.

**Speaker 5** 1:04:21

So I think, like, you can tell me if this makes sense to you, but I think like one reasonable middle ground between saying something like teachers should be ethical in their use of AI, which is not a very helpful thing, I agree, and saying these are the ethical uses of AI, and giving specifications is something like an articulation of what we take to be like. These are the factors like that a teacher ought to take into account in making a determination as to whether or not some specific use case is indeed an ethical application of AI. And the factors, I think, are arguable. So I think in the state, it's criteria for meaningfulness, but it might not be a philosophy, but it's like these are like several relevant factors that one ought to consider right? So let's

**Bethany Henderson** 1:05:03

use the ethical right? So it's important to us as a district,

**John Black** 1:05:15

can also be managed, okay, that's the next step. Okay, so we have at least a proposal for 123456789,

1:05:28

I can't count. 12345678,

**John Black** 1:05:35

about 15, pass, about 20, revise and about four Park the next after, after making some evaluation about that, and I'll put it to you this week, the next step will be the merging and cutting mud, right? And say, Okay, this is, this is the cut list that we're working with. This is good enough and, and that's what you'll hear about us. Hear from us. Here for me this week.

**David Berol** 1:06:06

So what's revised me? What does revise mean?

**John Black** 1:06:09

Revise means rise means it's it's fixable, right?

**David Berol** 1:06:13

It's who edits, whose job is that and when

1:06:20

he sounds like you want a job.

**David Berol** 1:06:23

I'll do whatever, but I just, I'm curious what I'm supposed to do on this in the next two weeks ago.

1:06:31

Okay, and if you revise it to be passable, then are we moving into way too many like I thought we were supposed to have six to 10.

**John Black** 1:06:40

So eight to 12, but, but who says eight to 12 was the right? It there is a certain, I mean, like 100 things, right? And if there and then we have to generate 100 trade off statements to go with them. Will, you know, we'll be intimate doing that, right? So we have that's, that's sort of our constraint, is time and then we got to get on with figuring out what policy areas are and how to write them. And so having enough coverage to the themes was a goal. We achieved that having understanding what are, are the the failure, you know, the pathologies here, and an effort to avoid them, right? And then what we'll see is a number of these things can be clustered and perhaps merged. Now you merge, if I merge them all into one principle, they would fail in vagueness, right? But you have enough merging that we capture the salience in all the different variations, but without creating too many principles to have to follow. Do we need

**Speaker 1** 1:07:43

to discuss the ones that seemingly up there passed, but yet, nobody in the

**David Berol** 1:07:49

committee voted to sponsor the AI ones? Of course, the AI ones passed.

**Speaker 1** 1:07:54

I mean, that just strikes me that we all voted for something seemingly the ones no human voted for, but yet and

**John Black** 1:08:01

immediately, I mean, it was aI with the context of our previous two meetings. Yeah, you have the option of leaving them behind or rewriting. And so you will have the option in the next survey to mark up and revise any of these or not, right? I mean, so if you want, if you, if you want, to just pick your one particular one that you're interested in, or your several, then do that and provide that back as as proposed revisions. I have to do it in this serial fashion. I can't get us together in a meeting, because this is the only you know format.

**Bethany Henderson** 1:08:39

John, may I suggest also that in that you give folks a chance to sort of vote their top 12, not necessarily in rank order, but just you can see, there's a heat map of where

**John Black** 1:08:47

that's that's the cap part, right? So it's revised, and essentially say, Yeah, which, which ones do we want to go with? We don't need all of them, right? We need enough of them.

**Bethany Henderson** 1:08:59

But I think even if your process, yeah, even if in this you get just like a sense of heat map, you might see a little so I don't want

**David Berol** 1:09:11

to wordsmith stuff that doesn't have enough support from the committee.

**John Black** 1:09:14

Okay, so, so last, last meeting we had, you know, it was assumptions, and you said, John, I think I voted I'll put a.on something. And it was, it was indicating I supported it when I don't support it, right? And I said it doesn't matter, because it was just it, was it valid indication of your of heat of interest, of that there's something there that we have to pull out. We're still in that, in that sort of phase where we're we're trying to pull it out from uncertainty about what to do and how

**David Berol** 1:09:50

to do it. I'm content to wait for further guidance

**John Black** 1:09:53

on what I'm supposed to wait for further guidance. Say again. So is that me the work Street. It's, it's, it's, it's mainly going to be a written delivery, if it's a presentation, because all of them, right, not research and share. But actually, if you have updates we're going to need, we would love to hear from you, right? But it's, it's, not going to be a 10 or 20 minute presentation, because I need to get I need the I need the work product from the work stream to get to policy formation.

**Speaker 5** 1:10:34

Okay, so you just need the written product by what is it going to be incorporated into the next meeting?

**John Black** 1:10:42

Yes, and so next week, I'll just say by by next Friday is when I'm going to need stuff by next Friday. Yeah, who

**David Berol** 1:10:53

is supposed to be like, presenting something?

**John Black** 1:10:56

Well, because, because, because I have to then merge all these things and then get it into a format where either you can present it, I can present it, or we just come on say, Oh, we've all or people can review it before the meeting. So your document, Tom and Elizabeth was very helpful, but it came in last night at 830 and it was hard for me to do much other than say, Well, let me just boil it down into a one pager,

**Bethany Henderson** 1:11:26

reminder that the town hall was after next Friday, but before the next

**John Black** 1:11:29

meeting, Town Hall is the sixth Monday

**Speaker 8** 1:11:35

March, the first town hall. We don't really going to be much. Yes, Monday sixth, yeah, oh no, no, no, that's our next net. Oh so materials by like

**John Black** 1:11:51

the Tuesday following I give you all a deadline. That's one that you can negotiate, but the main thing is that I have time to process it and interpolate it so So Tina, by that town meeting, we need to know how we're going to collect the inputs from the public, and I'll be there, right, but we still need some kind of way to do it.

**Bethany Henderson** 1:12:14

And I'd also love to chat the town hall community input group and stay for a couple minutes about the town hall

1:12:26

so we can have a common idea the meeting before. Oh, right now, while you're here,

**Bethany Henderson** 1:12:32

yeah, or someone can reach out to me or Jared, just, we want to make sure that we chat with you guys about, like, how town halls work. What's the best way to sort of, did the school district will promote it, obviously like, so should show them, like, morning announcements and that kind of thing. But also, when, when you were there, you know what sort of the here's typically a school board member will say, okay, you know, here's what we're doing and here's what we're thinking about. But then also, often, folks will ask questions, and so just to kind of get on the same page about how that's going to happen and who's going to be doing what. And I won't actually be at that one. I'll be the second one, but

**Thomas Colvin** 1:13:09

for these community events where, you know, like you said, We're encouraged to attend. But then there's also that rule where we can't talk about official community business if there's like, would just appreciate maybe emailing some guidance about ground rules for us as we participate in events.

**Bethany Henderson** 1:13:26

Yes, it is a publicly noticed event, and therefore you can talk to each other because it is a publicly noticed event. Oh, publicly noticed committee meeting, but more than two members of any board or entity can show up at publicly know

**John Black** 1:13:41

this defense. So there we're attending as members of the public who also happen to be serving on the

**Bethany Henderson** 1:13:47

committee, but I will, I will have sent out, okay, because there is a, I don't

**Speaker 1** 1:13:52

know if you have seen it, there is a group of concerned. What? Group of concerned parents meeting at the library. Are we allowed

**Thomas Colvin** 1:14:14

to go to that interact with that? Two of you, two, you're on that list. My wife is on that list. So I was like, I'm not gonna write no more than two of us,

**Bethany Henderson** 1:14:26

you can be on the list. It's a broadcast ad, right?

**John Black** 1:14:29

And, in fact, we are encouraged to, because that's part of our research.

**Bethany Henderson** 1:14:32

But in terms of, it's not a public event, but it is a public event, if it is a publicly noticed event that is different, if it is just like a community event, you can go listen, but really, in the perfect world, no more than two of you would be physically present, or whatever the physical presence is, okay, but I will get clear guidance,

**Thomas Colvin** 1:15:01

logical, okay. It's logical not being able to like, I am not used to this. I'm glad the federal government doesn't work.

1:15:11

It's really hard similar.

**Thomas Colvin** 1:15:15

I mean, I used to do inter agency work back you can get around Packer. I've got a rough pack of

**John Black** 1:15:25

how sausage is made.

1:15:32

Owe you a report for two

**Speaker 2** 1:15:33

weeks on something you will, you will. It will all become clear. Thank you. Tom, yeah,

**Speaker 9** 1:15:46

you can't, it's a state, yeah, but it's like.
